{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.io import imread\n",
    "from skimage import transform\n",
    "from tqdm import trange, tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1517/1517 [05:34<00:00,  4.40it/s]\n"
     ]
    }
   ],
   "source": [
    "input_dir = './cancer_data/inputs/'\n",
    "output_dir = './cancer_data/outputs/'\n",
    "\n",
    "files = os.listdir(input_dir)\n",
    "train_images, train_label, test_images, test_label = [], [], [], []\n",
    "for f in tqdm(files):\n",
    "    if 'train' in f:\n",
    "        train_images.append(transform.resize(imread(input_dir + f), (512,512,3), mode='constant'))\n",
    "        train_label.append(transform.resize(imread(output_dir + f), (512,512), mode='constant'))\n",
    "    else:\n",
    "        test_images.append(transform.resize(imread(input_dir + f), (512,512,3), mode='constant'))\n",
    "        test_label.append(transform.resize(imread(output_dir + f), (512,512), mode='constant'))\n",
    "\n",
    "#whiten the data\n",
    "train_images = train_images - np.mean( train_images )\n",
    "train_images = train_images / np.sqrt( np.var( train_images ) )\n",
    "\n",
    "test_images = test_images - np.mean( test_images )\n",
    "test_images = test_images / np.sqrt( np.var( test_images ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def conv( x, filter_size=3, stride=2, num_filters=64, is_output=False, name=\"conv\" ):\n",
    "    filter_height, filter_width = filter_size, filter_size\n",
    "    in_channels = x.get_shape().as_list()[-1]\n",
    "    out_channels = num_filters\n",
    "    \n",
    "    with tf.variable_scope(name) as scope:\n",
    "        W = tf.get_variable(\"{}_W\".format(name), shape=[filter_height, filter_width, in_channels, out_channels],\n",
    "                            initializer = tf.contrib.layers.variance_scaling_initializer())\n",
    "        b = tf.get_variable(\"{}_b\".format(name), shape=[out_channels],\n",
    "                            initializer = tf.contrib.layers.variance_scaling_initializer())\n",
    "        conv = tf.nn.conv2d(x, W, [1, stride, stride, 1], padding=\"SAME\")\n",
    "        out = tf.nn.bias_add(conv, b)\n",
    "        \n",
    "        if not is_output:\n",
    "            out = tf.nn.relu(out)\n",
    "            \n",
    "    return out\n",
    "\n",
    "def convt( x, out_shape, filter_size=8, stride=2, is_output=False, name=\"convt\" ):\n",
    "    filter_height, filter_width = filter_size, filter_size\n",
    "    in_channels = x.get_shape().as_list()[-1]\n",
    "\n",
    "    with tf.variable_scope( name ):\n",
    "        W = tf.get_variable( \"{}_W\".format(name), shape=[filter_height, filter_width, out_shape[-1], in_channels],\n",
    "                            initializer = tf.contrib.layers.variance_scaling_initializer() )\n",
    "        b = tf.get_variable( \"{}_b\".format(name), shape=[out_shape[-1]],\n",
    "                            initializer = tf.contrib.layers.variance_scaling_initializer() )\n",
    "        conv = tf.nn.conv2d_transpose( x, W, out_shape, [1, stride, stride, 1], padding=\"SAME\" )\n",
    "        out = tf.nn.bias_add( conv, b )\n",
    "        if not is_output:\n",
    "            out =  tf.nn.relu(out)\n",
    "\n",
    "    return out\n",
    "\n",
    "def fc( x, out_size=50, is_output=False, name=\"fc\" ):\n",
    "    in_size = x.get_shape().as_list()[0]\n",
    "    \n",
    "    with tf.variable_scope(name) as scope:\n",
    "        W = tf.get_variable(\"{}_W\".format(name), shape=[out_size, in_size], initializer = tf.contrib.layers.variance_scaling_initializer())\n",
    "        b = tf.get_variable(\"{}_b\".format(name), shape=[out_size, 1], initializer = tf.contrib.layers.variance_scaling_initializer())\n",
    "        \n",
    "        out = tf.matmul(W, x)\n",
    "        out = out + b\n",
    "        \n",
    "        if not is_output:\n",
    "            out = tf.nn.relu(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Topology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "input_data = tf.placeholder(tf.float32, [1,512,512,3])\n",
    "output_data = tf.placeholder(tf.float32, [1, 512, 512, 2])\n",
    "\n",
    "h0 = conv(input_data, stride=1, name=\"h0\") #[1, 32, 32, 64]\n",
    "h1 = conv(h0, stride=2, name=\"h1\") #[1, 16, 16, 64]\n",
    "h2 = conv(h1, num_filters=32, name=\"h2\") #[1, 8, 8, 32]\n",
    "flat = tf.reshape(h2, [1*8*8*32, 1]) #[1*8*8*32,1]\n",
    "# fc0 = fc(flat, name=\"fc0\") #[50,1]\n",
    "logits= fc(flat, out_size = 10, is_output=True, name=\"output\") # [10,1]\n",
    "\n",
    "\n",
    "with tf.name_scope( \"loss_function\" ) as scope:\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=tf.transpose(logits), labels=y_true)[0]\n",
    "\n",
    "with tf.name_scope( \"accuracy\" ) as scope:\n",
    "    correct_prediction = tf.equal( y_true, tf.argmax(logits,0) )\n",
    "    accuracy = tf.reduce_mean( tf.cast(correct_prediction, tf.float32) )\n",
    "\n",
    "train_step = tf.train.AdamOptimizer( 0.001 ).minimize( cross_entropy )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "train_writer = tf.summary.FileWriter( \"./tflogs\", sess.graph )\n",
    " \n",
    "tf.summary.scalar('cross_entropy', cross_entropy)\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "tmp_loss = 0.0\n",
    "accuracy_li = []\n",
    "loss_li = []\n",
    "for i in range( 100000 ):\n",
    "    ind_train = np.random.randint(len(train_features))\n",
    "\n",
    "    image_train = train_features[ind_train]\n",
    "    label_train = train_labels[ind_train]\n",
    "\n",
    "    acc_test = 0\n",
    "    \n",
    "    _, loss = sess.run( [train_step, cross_entropy], feed_dict={input_data: image_train.reshape([1,32,32,3]), y_true: label_train})\n",
    "    tmp_loss += loss\n",
    "    \n",
    "    if i % 1000 == 0 and i != 0:\n",
    "        for k in range(len(test_features)):\n",
    "            acc_test += sess.run( accuracy, feed_dict={input_data: test_features[k].reshape([1,32,32,3]), y_true: test_labels[k]})\n",
    "\n",
    "    if i % 1000 == 0 and i != 0:\n",
    "        print( \"i:{} test_acc:{} loss:{}\".format( i, acc_test/len(test_features), tmp_loss ) )\n",
    "        accuracy_li.append(acc_test/len(test_features))\n",
    "        loss_li.append(tmp_loss)\n",
    "        tmp_loss = 0.0\n",
    "\n",
    "#saver.save( sess, './tf_logs/model.ckpt' )\n",
    "train_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(accuracy_li)\n",
    "plt.title(\"Accuracy\")\n",
    "plt.xlabel(\"in thousands of iterations\")\n",
    "plt.ylabel(\"% correct\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(loss_li)\n",
    "plt.title(\"Loss\")\n",
    "plt.xlabel(\"in thousands of iterations\")\n",
    "plt.ylabel(\"average loss for 1000 iterations\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
