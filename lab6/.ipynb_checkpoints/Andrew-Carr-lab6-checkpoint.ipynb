{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.io import imread\n",
    "from skimage import transform\n",
    "from tqdm import trange, tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1517 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'io' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-a9e99433b035>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mtrain_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'constant'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mtest_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'constant'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mtest_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'constant'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'io' is not defined"
     ]
    }
   ],
   "source": [
    "input_dir = './cancer_data/inputs/'\n",
    "output_dir = './cancer_data/outputs/'\n",
    "\n",
    "files = os.listdir(input_dir)\n",
    "train_images, train_label, test_images, test_label = [], [], [], []\n",
    "for f in tqdm(files):\n",
    "    if 'train' in f:\n",
    "        train_images.append(transform.resize(imread(input_dir + f), (512,512,3), mode='constant'))\n",
    "        train_label.append(transform.resize(imread(output_dir + f), (512,512), mode='constant'))\n",
    "    else:\n",
    "        test_images.append(transform.resize(imread(input_dir + f), (512,512,3), mode='constant'))\n",
    "        test_label.append(transform.resize(imread(output_dir + f), (512,512), mode='constant'))\n",
    "\n",
    "#whiten the data\n",
    "train_images = train_images - np.mean( train_images )\n",
    "train_images = train_images / np.sqrt( np.var( train_images ) )\n",
    "\n",
    "test_images = test_images - np.mean( test_images )\n",
    "test_images = test_images / np.sqrt( np.var( test_images ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def conv( x, filter_size=3, stride=2, num_filters=64, is_output=False, name=\"conv\" ):\n",
    "    filter_height, filter_width = filter_size, filter_size\n",
    "    in_channels = x.get_shape().as_list()[-1]\n",
    "    out_channels = num_filters\n",
    "    \n",
    "    with tf.variable_scope(name) as scope:\n",
    "        W = tf.get_variable(\"{}_W\".format(name), shape=[filter_height, filter_width, in_channels, out_channels],\n",
    "                            initializer = tf.contrib.layers.variance_scaling_initializer())\n",
    "        b = tf.get_variable(\"{}_b\".format(name), shape=[out_channels],\n",
    "                            initializer = tf.contrib.layers.variance_scaling_initializer())\n",
    "        conv = tf.nn.conv2d(x, W, [1, stride, stride, 1], padding=\"SAME\")\n",
    "        out = tf.nn.bias_add(conv, b)\n",
    "        \n",
    "        if not is_output:\n",
    "            out = tf.nn.relu(out)\n",
    "            \n",
    "    return out\n",
    "\n",
    "def convt( x, out_shape, filter_size=8, stride=2, is_output=False, name=\"convt\" ):\n",
    "    filter_height, filter_width = filter_size, filter_size\n",
    "    in_channels = x.get_shape().as_list()[-1]\n",
    "\n",
    "    with tf.variable_scope( name ):\n",
    "        W = tf.get_variable( \"{}_W\".format(name), shape=[filter_height, filter_width, out_shape[-1], in_channels],\n",
    "                            initializer = tf.contrib.layers.variance_scaling_initializer() )\n",
    "        b = tf.get_variable( \"{}_b\".format(name), shape=[out_shape[-1]],\n",
    "                            initializer = tf.contrib.layers.variance_scaling_initializer() )\n",
    "        conv = tf.nn.conv2d_transpose( x, W, out_shape, [1, stride, stride, 1], padding=\"SAME\" )\n",
    "        out = tf.nn.bias_add( conv, b )\n",
    "        if not is_output:\n",
    "            out =  tf.nn.relu(out)\n",
    "\n",
    "    return out\n",
    "\n",
    "def fc( x, out_size=50, is_output=False, name=\"fc\" ):\n",
    "    in_size = x.get_shape().as_list()[0]\n",
    "    \n",
    "    with tf.variable_scope(name) as scope:\n",
    "        W = tf.get_variable(\"{}_W\".format(name), shape=[out_size, in_size], initializer = tf.contrib.layers.variance_scaling_initializer())\n",
    "        b = tf.get_variable(\"{}_b\".format(name), shape=[out_size, 1], initializer = tf.contrib.layers.variance_scaling_initializer())\n",
    "        \n",
    "        out = tf.matmul(W, x)\n",
    "        out = out + b\n",
    "        \n",
    "        if not is_output:\n",
    "            out = tf.nn.relu(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U Network - may not use this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unet_model(input_data, image_rows=512, image_cols=512):\n",
    "    conv1a = conv(input_data, filter_size=3, stride=1, num_filters=64, name='conv1a')\n",
    "    conv1b = conv( conv1a, filter_size=3, stride=1, num_filters=64, name=\"conv1b\")\n",
    "    pool12 = tf.nn.max_pool(conv1b, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME', name=\"pool12\")\n",
    "    \n",
    "    conv2a = conv(pool12, filter_size=3, stride=1, num_filters=128, name=\"conv2a\")\n",
    "    conv2b = conv(conv2a, filter_size=3, stride=1, num_filters=128, name=\"conv2b\")\n",
    "    pool23 = tf.nn.max_pool(conv2b, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME', name='pool23')\n",
    "        \n",
    "    conv3a = conv(pool23, filter_size=3, stride=1, num_filters=256, name='conv3a')\n",
    "    conv3b = conv(conv3a, filter_size=3, stride=1, num_filters=256, name='conv3b')\n",
    "    pool34 = tf.nn.max_pool(conv3b, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME', name='pool34')\n",
    "    \n",
    "    conv4a = conv(pool34, filter_size=3, stride=1, num_filters=512, name='conv4a')\n",
    "    conv4b = conv(conv4a, filter_size=3, stride=1, num_filters=512, name='conv4b')\n",
    "    pool45 = tf.nn.max_pool(conv4b, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME', name='pool45')\n",
    "    \n",
    "    # Buttom of the U\n",
    "    \n",
    "    conv5a = conv(pool45, filter_size=3, stride=1, num_filters=1024, name='conv5a')\n",
    "    conv5b = conv(conv5a, filter_size=3, stride=1, num_filters=1024, name='conv5b')\n",
    "    \n",
    "    # Turn around\n",
    "    \n",
    "    pool54 = convt(conv5b, [1,64,64,512], filter_size=2, stride=2, name=\"pool54\")\n",
    "    pool54c = tf.concat([pool54, conv4b], 3)\n",
    "    \n",
    "    conv4ci = conv(pool54c, filter_size=3, stride=1, num_filters=512, name='conv4ci')\n",
    "    conv4di = conv(conv4ci, filter_size=3, stride=1, num_filters=512, name='conv4di')\n",
    "    \n",
    "    pool43 = convt(conv4di, [1,128,128,256], filter_size=2, stride=2, name='pool43')\n",
    "    pool43c = tf.concat([pool43, conv3b], 3)\n",
    "    \n",
    "    conv3ci = conv(pool43c, filter_size=3, stride=1, num_filters=256, name='conv3ci')\n",
    "    conv3di = conv(conv3ci, filter_size=3, stride=1, num_filters=256, name='conv3di')\n",
    "    \n",
    "    pool32 = convt(conv3di, [1,256,256,128], filter_size=2, stride=2, name='pool32')\n",
    "    pool32c = tf.concat([pool32, conv2b], 3)\n",
    "    \n",
    "    conv2ci = conv(pool32c, filter_size=3, stride=1, num_filters=128, name='conv2ci')\n",
    "    conv2di = conv(conv2ci, filter_size=3, stride=1, num_filters=128, name='conv2di')\n",
    "    \n",
    "    pool21 = convt(conv2di, [1,512,512,64], filter_size=2, stride=2, name='pool21')\n",
    "    pool21c = tf.concat([pool21, conv1b], 3)\n",
    "    \n",
    "    conv1ci = conv(pool21c, filter_size=3, stride=1, num_filters=64, name='conv1ci')\n",
    "    conv1di = conv(conv1ci, filter_size=3, stride=1, num_filters=64, name='conv1di')\n",
    "    logits = conv(conv1di, filter_size=1, stride=1, num_filters=1, is_out=True, name='logits')\n",
    "    \n",
    "    return logits\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simpler Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "input_data = tf.placeholder(tf.float32, [1,512,512,3])\n",
    "output_data = tf.placeholder(tf.float32, [1, 512, 512, 2])\n",
    "\n",
    "h0 = conv(input_data, stride=1, name=\"h0\") #[1, 32, 32, 64]\n",
    "h1 = conv(h0, stride=2, name=\"h1\") #[1, 16, 16, 64]\n",
    "h2 = conv(h1, num_filters=32, name=\"h2\") #[1, 8, 8, 32]\n",
    "flat = tf.reshape(h2, [1*8*8*32, 1]) #[1*8*8*32,1]\n",
    "# fc0 = fc(flat, name=\"fc0\") #[50,1]\n",
    "logits= fc(flat, out_size = 10, is_output=True, name=\"output\") # [10,1]\n",
    "\n",
    "\n",
    "with tf.name_scope( \"loss_function\" ) as scope:\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=tf.transpose(logits), labels=y_true)[0]\n",
    "\n",
    "with tf.name_scope( \"accuracy\" ) as scope:\n",
    "    correct_prediction = tf.equal( y_true, tf.argmax(logits,0) )\n",
    "    accuracy = tf.reduce_mean( tf.cast(correct_prediction, tf.float32) )\n",
    "\n",
    "train_step = tf.train.AdamOptimizer( 0.001 ).minimize( cross_entropy )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "train_writer = tf.summary.FileWriter( \"./tflogs\", sess.graph )\n",
    " \n",
    "tf.summary.scalar('cross_entropy', cross_entropy)\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "tmp_loss = 0.0\n",
    "accuracy_li = []\n",
    "loss_li = []\n",
    "for i in range( 100000 ):\n",
    "    ind_train = np.random.randint(len(train_features))\n",
    "\n",
    "    image_train = train_features[ind_train]\n",
    "    label_train = train_labels[ind_train]\n",
    "\n",
    "    acc_test = 0\n",
    "    \n",
    "    _, loss = sess.run( [train_step, cross_entropy], feed_dict={input_data: image_train.reshape([1,32,32,3]), y_true: label_train})\n",
    "    tmp_loss += loss\n",
    "    \n",
    "    if i % 1000 == 0 and i != 0:\n",
    "        for k in range(len(test_features)):\n",
    "            acc_test += sess.run( accuracy, feed_dict={input_data: test_features[k].reshape([1,32,32,3]), y_true: test_labels[k]})\n",
    "\n",
    "    if i % 1000 == 0 and i != 0:\n",
    "        print( \"i:{} test_acc:{} loss:{}\".format( i, acc_test/len(test_features), tmp_loss ) )\n",
    "        accuracy_li.append(acc_test/len(test_features))\n",
    "        loss_li.append(tmp_loss)\n",
    "        tmp_loss = 0.0\n",
    "\n",
    "#saver.save( sess, './tf_logs/model.ckpt' )\n",
    "train_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(accuracy_li)\n",
    "plt.title(\"Accuracy\")\n",
    "plt.xlabel(\"in thousands of iterations\")\n",
    "plt.ylabel(\"% correct\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(loss_li)\n",
    "plt.title(\"Loss\")\n",
    "plt.xlabel(\"in thousands of iterations\")\n",
    "plt.ylabel(\"average loss for 1000 iterations\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
